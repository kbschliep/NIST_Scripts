{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "======================================================================================\n",
    "03. Translation and the NumpyTranslator\n",
    "======================================================================================\n",
    "\n",
    "**Suhas Somnath**\n",
    "\n",
    "8/8/2017\n",
    "\n",
    "This document illustrates an example of extracting data out of proprietary raw data files and writing the information\n",
    "into a **Universal Spectroscopy and Imaging Data (USID)** HDF5 file (referred to as a **h5USID** file) using the\n",
    "``pyUSID.NumpyTranslator``\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "Before any data analysis, we need to access data stored in the raw file(s) generated by the microscope. Often, the\n",
    "data and parameters in these files are **not** straightforward to access. In certain cases, additional / dedicated\n",
    "software packages are necessary to access the data while in many other cases, it is possible to extract the necessary\n",
    "information from built-in **numpy** or similar python packages included with **anaconda**.\n",
    "\n",
    "The USID model aims to make data access, storage, curation, etc. simply by storing the data along with all\n",
    "relevant parameters in a single file (HDF5 for now).\n",
    "\n",
    "The process of copying data from the original format to **h5USID** files is called\n",
    "**Translation** and the classes available in pyUSID and children packages such as pycroscopy that perform these\n",
    "operation are called **Translators**\n",
    "\n",
    "Simply put, so long as one has the metadata and the actual data extracted from the raw data file,\n",
    "the ``pyUSID.NumpyTranslator`` will correctly write the contents to a h5UID / HDF5 file.\n",
    "Note that the complexity or size of the raw data may necessitate a custom Translator class. However, the rough process\n",
    "of translation is the same regardless of the origin, complexity, or size of the raw data:\n",
    "\n",
    "* Investigating how to open the proprietary raw data file\n",
    "* Reading the metadata\n",
    "* Extracting the data\n",
    "* Writing to h5USID file\n",
    "\n",
    "The goal of this document is to demonstrate how one would extract data and parameters from a Scanning Tunnelling\n",
    "Spectroscopy (STS) raw data file obtained from an Omicron Scanning Tunneling Microscope (STM) into a h5USID file.\n",
    "\n",
    "While there is an `AscTranslator <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/omicron_asc.py>`_\n",
    "available in our sister-package - ``pycroscopy`` that can translate these files in just a **single** line,\n",
    "we will intentionally assume that no such translator is available. Using a handful of useful functions in pyUSID,\n",
    "we will translate the files from the source **.asc** format to h5USID files in just a few lines.\n",
    "\n",
    "The same methodology can be used to translate other data formats\n",
    "\n",
    "Recommended pre-requisite reading\n",
    "---------------------------------\n",
    "\n",
    "Before proceeding with this example, we recommend reading the previous documents to learn more about:\n",
    "\n",
    "* `Universal Spectroscopic and Imaging Data (USID) model </../../../USID/usid_model.html>`_\n",
    "\n",
    ".. tip::\n",
    "    You can download and run this document as a Jupyter notebook using the link at the bottom of this page.\n",
    "\n",
    "Import all necessary packages\n",
    "-----------------------------\n",
    "There are a few setup procedures that need to be followed before any code is written. In this step, we simply load a\n",
    "few python packages that will be necessary in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure python 3 compatibility:\n",
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "# The package for accessing files in directories, etc.:\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Warning package in case something goes wrong\n",
    "from warnings import warn\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install(package):\n",
    "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "# Package for downloading online files:\n",
    "try:\n",
    "    # This package is not part of anaconda and may need to be installed.\n",
    "    import wget\n",
    "except ImportError:\n",
    "    warn('wget not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install(wget)\n",
    "    import wget\n",
    "\n",
    "# The mathematical computation package:\n",
    "import numpy as np\n",
    "\n",
    "# The package used for creating and manipulating HDF5 files:\n",
    "import h5py\n",
    "\n",
    "# Packages for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Finally import pyUSID:\n",
    "try:\n",
    "    import pyUSID as usid\n",
    "except ImportError:\n",
    "    warn('pyUSID not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install('pyUSID')\n",
    "    import pyUSID as usid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0. Procure the Raw Data file\n",
    "=================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the compressed data file from Github:\n",
    "url = 'https://raw.githubusercontent.com/pycroscopy/pyUSID/master/data/STS.zip'\n",
    "zip_path = 'STS.zip'\n",
    "if os.path.exists(zip_path):\n",
    "    os.remove(zip_path)\n",
    "_ = wget.download(url, zip_path, bar=None)\n",
    "\n",
    "zip_path = os.path.abspath(zip_path)\n",
    "# figure out the folder to unzip the zip file to\n",
    "folder_path, _ = os.path.split(zip_path)\n",
    "zip_ref = zipfile.ZipFile(zip_path, 'r')\n",
    "# unzip the file\n",
    "zip_ref.extractall(folder_path)\n",
    "zip_ref.close()\n",
    "# delete the zip file\n",
    "os.remove(zip_path)\n",
    "\n",
    "data_file_path = 'STS.asc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Exploring the Raw Data File\n",
    "====================================\n",
    "\n",
    "Inherently, one may not know how to read these ``.asc`` files. One option is to try and read the file as a text file\n",
    "one line at a time.\n",
    "\n",
    "It turns out that these ``.asc`` files are effectively the standard ``ASCII`` text files.\n",
    "\n",
    "Here is how we tested to see if the ``asc`` files could be interpreted as text files. Below, we read just the first 10\n",
    "lines in the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file_path, 'r') as file_handle:\n",
    "    for lin_ind in range(10):\n",
    "        print(file_handle.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Loading the data\n",
    "=========================\n",
    "Now that we know that these files are simple text files, we can manually go through the file to find out which lines\n",
    "are important, at what lines the data starts etc.\n",
    "Manual investigation of such ``.asc`` files revealed that these files are always formatted in the same way. Also, they\n",
    "contain parameters in the first ``403`` lines and then contain data which is arranged as one pixel per row.\n",
    "STS experiments result in 3 dimensional datasets ``(X, Y, current)``. In other words, a 1D array of current data (as a\n",
    "function of excitation bias) is sampled at every location on a two dimensional grid of points on the sample.\n",
    "By knowing where the parameters are located and how the data is structured, it is possible to extract the necessary\n",
    "information from these files.\n",
    "Since we know that the data sizes (<200 MB) are much smaller than the physical memory of most computers, we can start\n",
    "by safely loading the contents of the entire file to memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the raw data into memory\n",
    "file_handle = open(data_file_path, 'r')\n",
    "string_lines = file_handle.readlines()\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Read the parameters\n",
    "==============================\n",
    "The parameters in these files are present in the first few lines of the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading parameters stored in the first few rows of the file\n",
    "parm_dict = dict()\n",
    "for line in string_lines[3:17]:\n",
    "    line = line.replace('# ', '')\n",
    "    line = line.replace('\\n', '')\n",
    "    temp = line.split('=')\n",
    "    test = temp[1].strip()\n",
    "    try:\n",
    "        test = float(test)\n",
    "        # convert those values that should be integers:\n",
    "        if test % 1 == 0:\n",
    "            test = int(test)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    parm_dict[temp[0].strip()] = test\n",
    "\n",
    "# Print out the parameters extracted\n",
    "for key in parm_dict.keys():\n",
    "    print(key, ':\\t', parm_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.a Prepare to read the data\n",
    "==================================\n",
    "Before we read the data, we need to make an empty array to store all this data. In order to do this, we need to read\n",
    "the dictionary of parameters we made in step 2 and extract necessary quantities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = int(parm_dict['y-pixels'])\n",
    "num_cols = int(parm_dict['x-pixels'])\n",
    "num_pos = num_rows * num_cols\n",
    "spectra_length = int(parm_dict['z-points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.b Read the data\n",
    "========================\n",
    "Data is present after the first ``403`` lines of parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_headers = len(string_lines) - num_pos\n",
    "num_headers = 403\n",
    "\n",
    "# Extract the STS data from subsequent lines\n",
    "raw_data_2d = np.zeros(shape=(num_pos, spectra_length), dtype=np.float32)\n",
    "for line_ind in range(num_pos):\n",
    "    this_line = string_lines[num_headers + line_ind]\n",
    "    string_spectrum = this_line.split('\\t')[:-1]  # omitting the new line\n",
    "    raw_data_2d[line_ind] = np.array(string_spectrum, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4.a Preparing some necessary parameters\n",
    "=============================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_v = 1  # This is the one parameter we are not sure about\n",
    "\n",
    "folder_path, file_name = os.path.split(data_file_path)\n",
    "file_name = file_name[:-4] + '_'\n",
    "\n",
    "# Generate the x / voltage / spectroscopic axis:\n",
    "volt_vec = np.linspace(-1 * max_v, 1 * max_v, spectra_length)\n",
    "\n",
    "h5_path = os.path.join(folder_path, file_name + '.h5')\n",
    "\n",
    "sci_data_type = 'STS'\n",
    "quantity = 'Current'\n",
    "units = 'nA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4.b. Defining the Dimensions\n",
    "===================================\n",
    "Position and spectroscopic dimensions need to defined using ``Dimension`` objects. Remember that the position and\n",
    "spectroscopic dimensions need to be specified in the correct order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dims = [usid.write_utils.Dimension('X', 'a. u.', parm_dict['x-pixels']),\n",
    "            usid.write_utils.Dimension('Y', 'a. u.', parm_dict['y-pixels'])]\n",
    "spec_dims = usid.write_utils.Dimension('Bias', 'V', volt_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4.c. Calling the NumpyTranslator to create the h5USID file\n",
    "==========================================================================\n",
    "The NumpyTranslator simplifies the creation of h5USID files. It handles the HDF5 file creation,\n",
    "HDF5 dataset creation and writing, creation of ancillary HDF5 datasets, group creation, writing parameters, linking\n",
    "ancillary datasets to the main dataset etc. With a single call to the NumpyTranslator, we complete the translation\n",
    "process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = usid.NumpyTranslator()\n",
    "h5_path = tran.translate(h5_path, sci_data_type, raw_data_2d,  quantity, units,\n",
    "                         pos_dims, spec_dims, translator_name='Omicron_ASC_Translator', parm_dict=parm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on translation\n",
    "=====================\n",
    "* Steps 1-3 would be performed anyway in order to begin data analysis\n",
    "* The actual procedure for translation to h5USID is reduced to just 3-4 lines in step 4.\n",
    "* A modular / formal version of this translator has been implemented as a class in pycroscopy as the\n",
    "  `AscTranslator <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/omicron_asc.py>`_.\n",
    "  This custom translator packages the same code used above into functions that focus on the individual tasks such\n",
    "  as extracting parameters, reading data, and writing to h5USID. The ``NumpyTranslator`` uses the\n",
    "  ``pyUSID.hdf_utils.write_main_dataset()`` function underneath to write its data. You can learn more about lower-\n",
    "  level file-writing functions in another tutorial on `writing <./plot_hdf_utils_write.html>`_ h5USID files.\n",
    "* There are many benefits to writing such a formal Translator class instead of standalone scripts like this including:\n",
    "\n",
    "  * Unlike such a stand-alone script, a Translator class in the package can be used by everyone repeatedly\n",
    "  * The custom Translator class can ensure consistency when translating multiple files. \n",
    "  * A single, robust Translator class can handle the finer variations / modes in the data. See the\n",
    "    `IgorIBWTranslator <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/igor_ibw.py>`_\n",
    "    as an example.\n",
    "\n",
    "* While this approach is feasible and encouraged for simple and small data, it may be necessary to use lower level\n",
    "  calls to write efficient translators. As an example, please see the `BEPSndfTranslator\n",
    "  <https://github.com/pycroscopy/pycroscopy/blob/master/pycroscopy/io/translators/beps_ndf.py>`_\n",
    "* We have found python packages online to open a few proprietary file formats and have written translators using these\n",
    "  packages. If you are having trouble reading the data in your files and cannot find any packages online, consider \n",
    "  contacting the manufacturer of the instrument which generated the data in the proprietary format for help.\n",
    "\n",
    "Verifying the newly written H5 file:\n",
    "====================================\n",
    "* We will only perform some simple and quick verification to show that the data has indeed been translated correctly.\n",
    "* Please see the next notebook in the example series to learn more about reading and accessing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_path, mode='r') as h5_file:\n",
    "    # See if a tree has been created within the hdf5 file:\n",
    "    usid.hdf_utils.print_tree(h5_file)\n",
    "\n",
    "    h5_main = h5_file['Measurement_000/Channel_000/Raw_Data']\n",
    "    usid.plot_utils.use_nice_plot_params()\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(11, 5))\n",
    "    spat_map = np.reshape(h5_main[:, 100], (100, 100))\n",
    "    usid.plot_utils.plot_map(axes[0], spat_map, origin='lower')\n",
    "    axes[0].set_title('Spatial map')\n",
    "    axes[0].set_xlabel('X')\n",
    "    axes[0].set_ylabel('Y')\n",
    "    axes[1].plot(np.linspace(-1.0, 1.0, h5_main.shape[1]),\n",
    "                 h5_main[250])\n",
    "    axes[1].set_title('IV curve at a single pixel')\n",
    "    axes[1].set_xlabel('Tip bias [V]')\n",
    "    axes[1].set_ylabel('Current [nA]')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "# Remove both the original and translated files:\n",
    "os.remove(h5_path)\n",
    "os.remove(data_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
